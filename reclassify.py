import subprocess
import re
import os
import requests
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

def download_txt(txt_url: str, local_path: str) -> bool:
    """ä»Githubä¸‹è½½TXTæ–‡ä»¶ï¼ˆé€‚é…ä»“åº“RAWåœ°å€ï¼‰"""
    try:
        response = requests.get(txt_url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()
        with open(local_path, 'w', encoding='utf-8', errors='ignore') as f:
            f.write(response.text)
        print(f"âœ… æˆåŠŸä¸‹è½½TXTæ–‡ä»¶ï¼š{txt_url}")
        return True
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{str(e)}")
        return False

def load_and_clean_txt(file_path: str) -> List[str]:
    """åŸºç¡€æ¸…ç†ï¼šåˆ é™¤å‰ä¸¤è¡Œã€ç§»é™¤-ç»„æ’­ã€è¿‡æ»¤ç©ºè¡Œ"""
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        lines = f.readlines()
    # åˆ é™¤å‰ä¸¤è¡Œï¼ˆç¡®ä¿å…¼å®¹è¡Œæ•°ä¸è¶³åœºæ™¯ï¼‰
    cleaned_lines = lines[2:] if len(lines) >= 2 else lines
    # æ¸…ç†å†…å®¹ï¼šç§»é™¤-ç»„æ’­ã€å»é‡ç©ºè¡Œã€stripé¦–å°¾ç©ºæ ¼
    cleaned_lines = [
        line.replace('-ç»„æ’­', '').strip() 
        for line in cleaned_lines 
        if line.strip() and not line.startswith('#EXTM3U')  # è¿‡æ»¤æ— æ•ˆç©ºè¡Œå’ŒM3Uå¤´
    ]
    return cleaned_lines

def parse_groups(cleaned_lines: List[str]) -> Dict[str, List[Tuple[str, str]]]:
    """è§£æåˆ†ç»„æ•°æ®ï¼škey=ç»„åï¼Œvalue=[(é¢‘é“åç§°, æ’­æ”¾åœ°å€), ...]"""
    groups: Dict[str, List[Tuple[str, str]]] = {}
    current_group = ""
    for line in cleaned_lines:
        if line.startswith('#genre#'):
            # æå–ç»„åï¼ˆå…¼å®¹é€—å·åˆ†éš”å’Œçº¯æ–‡æœ¬ç»„åï¼‰
            group_match = re.search(r'#genre#(.+?)(?:,|$)', line)
            current_group = group_match.group(1).strip() if group_match else ""
            if current_group and current_group not in groups:
                groups[current_group] = []
        else:
            # è§£æé¢‘é“ï¼ˆå…¼å®¹å¤šé€—å·åœºæ™¯ï¼Œå–ç¬¬ä¸€ä¸ªé€—å·åˆ†éš”ï¼‰
            if ',' in line and current_group:
                name, url = line.split(',', 1)
                name = name.strip()
                url = url.strip()
                if url.startswith(('http://', 'https://', 'rtsp://', 'rtmp://', 'm3u8://')):
                    groups[current_group].append((name, url))
    return groups

def check_url_validity(url: str, timeout: int = 5, retries: int = 1) -> bool:
    """ä¼˜åŒ–ffprobeæ£€æµ‹é€»è¾‘ï¼šè¶…æ—¶é‡è¯•+ç²¾ç®€å‚æ•°+é”™è¯¯æŠ‘åˆ¶"""
    cmd = [
        'ffprobe', '-v', 'panic',  # ä»…è¾“å‡ºä¸¥é‡é”™è¯¯ï¼ˆå‡å°‘æ—¥å¿—ï¼‰
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        '-timeout', f'{timeout * 1000000}',  # å¾®ç§’å•ä½
        '-rtsp_transport', 'tcp',  # RTSPåè®®å¼ºåˆ¶TCPï¼ˆæå‡å…¼å®¹æ€§ï¼‰
        url
    ]
    
    for attempt in range(retries + 1):
        try:
            result = subprocess.run(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                check=True, timeout=timeout + 3,  # é¢„ç•™ç¼“å†²æ—¶é—´
                encoding='utf-8', shell=False  # ç¦ç”¨shellï¼ˆå®‰å…¨+é€‚é…Githubç¯å¢ƒï¼‰
            )
            # éªŒè¯æ—¶é•¿ï¼ˆéç©º+æ•°å­—æ ¼å¼ï¼‰
            duration = result.stdout.strip()
            if duration and (duration.replace('.', '').isdigit()):
                return True
        except subprocess.TimeoutExpired:
            if attempt < retries:
                print(f"âš ï¸  åœ°å€è¶…æ—¶ï¼Œé‡è¯•ç¬¬{attempt + 1}æ¬¡ï¼š{url}")
                time.sleep(1)  # é‡è¯•é—´éš”
            else:
                print(f"âŒ åœ°å€è¶…æ—¶ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•ï¼‰ï¼š{url}")
        except (subprocess.CalledProcessError, Exception) as e:
            # å¿½ç•¥éè‡´å‘½é”™è¯¯ï¼ˆå¦‚åè®®ä¸æ”¯æŒï¼‰
            if attempt == retries:
                print(f"âŒ åœ°å€æ— æ•ˆï¼š{url}ï¼ˆé”™è¯¯ï¼š{str(e)[:50]}ï¼‰")
    return False

def filter_valid_groups(groups: Dict[str, List[Tuple[str, str]]], max_workers: int = 8) -> Dict[str, List[Tuple[str, str]]]:
    """å¤šçº¿ç¨‹æ‰¹é‡æ£€æµ‹ï¼ˆæå‡Githubè¿è¡Œæ•ˆç‡ï¼‰"""
    valid_groups: Dict[str, List[Tuple[str, str]]] = {}
    group_list = list(groups.items())
    
    print(f"ğŸ” å¼€å§‹æ£€æµ‹ {len(group_list)} ä¸ªåˆ†ç»„ï¼ˆæ¯ä¸ªåˆ†ç»„ä»…æ£€æµ‹ç¬¬ä¸€ä¸ªé¢‘é“ï¼Œ{max_workers}çº¿ç¨‹å¹¶è¡Œï¼‰")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ£€æµ‹ä»»åŠ¡
        future_to_group = {
            executor.submit(check_url_validity, channels[0][1]): (group_name, channels)
            for group_name, channels in group_list
            if channels  # è·³è¿‡ç©ºåˆ†ç»„
        }
        
        # å¤„ç†æ£€æµ‹ç»“æœ
        for future in as_completed(future_to_group):
            group_name, channels = future_to_group[future]
            try:
                is_valid = future.result()
                if is_valid:
                    valid_groups[group_name] = channels
                    print(f"âœ… æœ‰æ•ˆåˆ†ç»„ï¼š{group_name}ï¼ˆé¢‘é“æ•°ï¼š{len(channels)}ï¼‰")
            except Exception as e:
                print(f"âš ï¸  åˆ†ç»„æ£€æµ‹å¼‚å¸¸ {group_name}ï¼š{str(e)}")
    
    print(f"\nğŸ“Š æ£€æµ‹å®Œæˆï¼šæœ‰æ•ˆåˆ†ç»„ {len(valid_groups)}/{len(group_list)}")
    return valid_groups

def generate_flat_output(valid_groups: Dict[str, List[Tuple[str, str]]]) -> List[str]:
    """ç”Ÿæˆå¹³è¡¨ï¼šé¢‘é“åç§°,æ’­æ”¾åœ°å€$ç»„åï¼ˆå–æ¶ˆé‡åˆ†ç±»+ç§»é™¤M3Uå¤´ï¼‰"""
    flat_lines = []
    for group_name, channels in valid_groups.items():
        for name, url in channels:
            # ç»„åç‰¹æ®Šå­—ç¬¦è½¬ä¹‰ï¼ˆé¿å…åˆ†éš”ç¬¦å†²çªï¼‰
            safe_group = group_name.replace(',', 'ï¼Œ').replace('$', 'ï¿¥')
            flat_lines.append(f"{name},{url}${safe_group}")
    return flat_lines

def save_flat_result(lines: List[str], output_path: str = "reclassify.txt"):
    """ä¿å­˜å¹³è¡¨ç»“æœï¼ˆè¾“å‡ºæ–‡ä»¶åä¸ºreclassify.txtï¼Œæ— M3Uå¤´ï¼‰"""
    with open(output_path, 'w', encoding='utf-8') as f:
        for line in lines:
            f.write(f"{line}\n")
    print(f"\nğŸ“„ å¹³è¡¨ç»“æœå·²ä¿å­˜ï¼š{os.path.abspath(output_path)}ï¼ˆå…±{len(lines)}æ¡é¢‘é“ï¼‰")

if __name__ == "__main__":
    # é…ç½®å‚æ•°ï¼ˆGithubä»“åº“RAWåœ°å€ï¼Œç›´æ¥å¯è®¿é—®ï¼‰
    TXT_URL = "https://raw.githubusercontent.com/q1017673817/iptvz/main/zubo_all.txt"
    LOCAL_TXT_PATH = "zubo_all.txt"
    MAX_WORKERS = 8  # çº¿ç¨‹æ•°ï¼ˆé€‚é…Github Actionsèµ„æºï¼‰
    FFPROBE_TIMEOUT = 5  # æ£€æµ‹è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    FFPROBE_RETRIES = 1  # è¶…æ—¶é‡è¯•æ¬¡æ•°

    # 1. ä¸‹è½½TXTæ–‡ä»¶
    if not download_txt(TXT_URL, LOCAL_TXT_PATH):
        exit(1)

    # 2. åŸºç¡€æ¸…ç†
    print("\nğŸ”§ æ‰§è¡ŒåŸºç¡€æ¸…ç†...")
    cleaned_lines = load_and_clean_txt(LOCAL_TXT_PATH)
    print(f"ğŸ“¥ æ¸…ç†åæœ‰æ•ˆè¡Œæ•°ï¼š{len(cleaned_lines)}")

    # 3. è§£æåˆ†ç»„
    print("\nğŸ“Š è§£æåˆ†ç»„æ•°æ®...")
    groups = parse_groups(cleaned_lines)
    print(f"ğŸ“ˆ è§£æåˆ°åˆ†ç»„æ•°ï¼š{len(groups)}")

    # 4. å¤šçº¿ç¨‹è¿‡æ»¤æœ‰æ•ˆåˆ†ç»„
    print("\n" + "="*50)
    valid_groups = filter_valid_groups(groups, max_workers=MAX_WORKERS)
    print("="*50 + "\n")

    # 5. ç”Ÿæˆå¹³è¡¨
    print("ğŸ“ ç”Ÿæˆå¹³è¡¨æ•°æ®...")
    flat_lines = generate_flat_output(valid_groups)

    # 6. ä¿å­˜ç»“æœ
    save_flat_result(flat_lines)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼ˆå¯é€‰ï¼ŒGithub Actionsè‡ªåŠ¨æ¸…ç†ï¼‰
    if os.path.exists(LOCAL_TXT_PATH):
        os.remove(LOCAL_TXT_PATH)
        print(f"ğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶ï¼š{LOCAL_TXT_PATH}")
